from bs4 import BeautifulSoup
import requests
from datetime import datetime, timedelta
import feedparser
import time
from time import mktime
# ‚úÖ FIX: Import t·ª´ app.models thay v√¨ models
from models.article import Article, ArticleBlock
from models.response import APIResponse
from typing import List, Optional, Union
import pytz
import re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os
from google import genai
from google.genai import types
import wave
from pathlib import Path
import cloudinary
import cloudinary.uploader
import uuid
import json

class ArticleSummarizationService:
    # üîß Lazy loading - ch·ªâ load khi c·∫ßn
    _tokenizer = None
    _model = None
    _device = None
    
    @classmethod
    def _load_model(cls):
        """Lazy load model ƒë·ªÉ tr√°nh ch·∫≠m kh·ªüi ƒë·ªông"""
        if cls._model is None:
            print("ü§ñ Loading ViT5 summarization model...")
            cls._device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            cls._tokenizer = AutoTokenizer.from_pretrained("VietAI/vit5-base-vietnews-summarization")
            cls._model = AutoModelForSeq2SeqLM.from_pretrained("VietAI/vit5-base-vietnews-summarization")
            cls._model.to(cls._device)
            cls._model.eval()  # Set to evaluation mode
            print(f"‚úÖ Model loaded on {cls._device}")
    
    @classmethod
    def chunk_text(cls, text: str, max_tokens: int = 512) -> List[str]:
        """Chia text th√†nh chunks ph√π h·ª£p v·ªõi ViT5"""
        cls._load_model()
        
        # üîß C·∫£i thi·ªán vi·ªác split sentences
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # Skip sentences qu√° ng·∫Øn ho·∫∑c kh√¥ng c√≥ nghƒ©a
            if len(sentence.strip()) < 10:
                continue
                
            test_chunk = (current_chunk + " " + sentence).strip()
            
            # üîß Tokenize ƒë·ªÉ check length ch√≠nh x√°c
            token_count = len(cls._tokenizer.encode(test_chunk, add_special_tokens=True))
            
            if token_count > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk = test_chunk
        
        # Add chunk cu·ªëi
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    @classmethod
    def summarize_text(cls, text: str, max_length: int = 150) -> str:
        """T√≥m t·∫Øt text v·ªõi error handling"""
        try:
            cls._load_model()
            
            if not text or len(text.strip()) < 50:
                return "N·ªôi dung qu√° ng·∫Øn ƒë·ªÉ t√≥m t·∫Øt"
            
            # üîß Clean text tr∆∞·ªõc khi process
            cleaned_text = re.sub(r'\s+', ' ', text.strip())
            
            # üîß GI·∫¢M CHUNK SIZE ƒë·ªÉ nhanh h∆°n
            chunks = cls.chunk_text(cleaned_text, max_tokens=256)  # Gi·∫£m t·ª´ 512 xu·ªëng 256
            
            if not chunks:
                return "Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt"
            
            summaries = []
            
            for i, chunk in enumerate(chunks):
                try:
                    print(f"üìù Summarizing chunk {i+1}/{len(chunks)}...")
                    
                    # üîß Tokenize v·ªõi proper settings
                    inputs = cls._tokenizer(
                        chunk, 
                        return_tensors="pt", 
                        max_length=256,  # Gi·∫£m t·ª´ 512
                        truncation=True, 
                        padding=True
                    ).to(cls._device)
                    
                    # üîß Generate v·ªõi FASTER parameters
                    with torch.no_grad():
                        summary_ids = cls._model.generate(
                            inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            max_length=min(max_length, 100),  # Gi·∫£m max_length
                            min_length=20,                    # Gi·∫£m min_length
                            num_beams=3,                      # Gi·∫£m t·ª´ 4 xu·ªëng 2
                            length_penalty=1.2,               # Gi·∫£m t·ª´ 2.0
                            early_stopping=True,
                            no_repeat_ngram_size=2,
                            do_sample=False                   # Deterministic generation
                        )
                    
                    summary = cls._tokenizer.decode(
                        summary_ids[0], 
                        skip_special_tokens=True
                    ).strip()
                    
                    if summary and len(summary) > 10:
                        summaries.append(summary)
                        print(f"‚úÖ Chunk {i+1} summarized: {len(summary)} chars")
                    
                except Exception as e:
                    print(f"‚ùå Error summarizing chunk {i+1}: {e}")
                    continue
            
            if not summaries:
                return "Kh√¥ng th·ªÉ t√≥m t·∫Øt n·ªôi dung"
            
            # üîß Join v√† clean final summary
            final_summary = " ".join(summaries)
            final_summary = re.sub(r'\s+', ' ', final_summary).strip()
            
            print(f"üéØ Final summary: {len(final_summary)} chars")
            return final_summary
            
        except Exception as e:
            print(f"‚ùå Error in summarization: {e}")
            return "L·ªói khi t√≥m t·∫Øt n·ªôi dung"

    @classmethod
    def summarize_article(cls, article: Article, max_length: int = 200) -> str:
        """T√≥m t·∫Øt Article object"""
        if not article.blocks:
            return "Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt"
        
        # üîß Extract only paragraph content + filter quality
        paragraphs = []
        for block in article.blocks:
            if (block.type == 'paragraph' and 
                block.content and 
                len(block.content.strip()) > 20):  # Skip very short paragraphs
                paragraphs.append(block.content.strip())
        
        if not paragraphs:
            return "Kh√¥ng c√≥ ƒëo·∫°n vƒÉn ƒë·ªÉ t√≥m t·∫Øt"
        
        full_text = " ".join(paragraphs)
        
        # üîß Add context v·ªõi title
        if article.title:
            full_text = f"Ti√™u ƒë·ªÅ: {article.title}. N·ªôi dung: {full_text}"
        
        return cls.summarize_text(full_text, max_length)
    
    @classmethod
    def cleanup_model(cls):
        """Cleanup model ƒë·ªÉ gi·∫£i ph√≥ng memory"""
        if cls._model is not None:
            del cls._model
            cls._model = None
        if cls._tokenizer is not None:
            del cls._tokenizer
            cls._tokenizer = None
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        print("üßπ Model cleaned up")
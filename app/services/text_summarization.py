import hashlib
from bs4 import BeautifulSoup
import requests
from datetime import datetime, timedelta
import feedparser
import time
from time import mktime
from app.models.article import Article, ArticleBlock
from app.models.response import APIResponse
from typing import List, Optional, Union, Dict, Any
import pytz
import re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os
from google import genai
from google.genai import types
import wave
from pathlib import Path
import cloudinary
import cloudinary.uploader
import uuid
import json
from app.core.config import settings
from huggingface_hub import InferenceClient

class ArticleSummarizationService:
    # üîß Lazy loading - ch·ªâ load khi c·∫ßn
    _tokenizer = None
    _model = None
    _device = None
    _summary_cache = {}
    _ner_cache = {}
    _ner_client = None
    
    @classmethod
    def _load_model(cls):
        """Lazy load model ƒë·ªÉ tr√°nh ch·∫≠m kh·ªüi ƒë·ªông"""
        if cls._model is None:
            print("ü§ñ Loading ViT5 summarization model...")
            cls._device = torch.device("cpu")
            cls._tokenizer = AutoTokenizer.from_pretrained("VietAI/vit5-base-vietnews-summarization")
            cls._model = AutoModelForSeq2SeqLM.from_pretrained("VietAI/vit5-base-vietnews-summarization")
            quantization_config = torch.quantization.quantize_dynamic(
                {torch.nn.Linear}, dtype=torch.qint8
            )
            cls._model = AutoModelForSeq2SeqLM.from_pretrained(
                "VietAI/vit5-base-vietnews-summarization",
                device_map="auto",
                quantization_config=quantization_config,
                low_cpu_mem_usage=True
            )
    
    @classmethod
    def _get_ner_client(cls):
        """Kh·ªüi t·∫°o client cho NER API"""
        if cls._ner_client is None:
            print("ü§ñ Initializing NER client...")
            # L·∫•y API key t·ª´ settings n·∫øu c√≥
            api_key = getattr(settings, "HUGGINGFACE_API_KEY", "")
            cls._ner_client = InferenceClient(
                provider="hf-inference",
                api_key=api_key
            )
        return cls._ner_client
    
    @classmethod
    def chunk_text(cls, text: str, max_tokens: int = 512) -> List[str]:
        """Chia text th√†nh chunks ph√π h·ª£p v·ªõi ViT5"""
        cls._load_model()
        
        # üîß C·∫£i thi·ªán vi·ªác split sentences
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # Skip sentences qu√° ng·∫Øn ho·∫∑c kh√¥ng c√≥ nghƒ©a
            if len(sentence.strip()) < 10:
                continue
                
            test_chunk = (current_chunk + " " + sentence).strip()
            
            # üîß Tokenize ƒë·ªÉ check length ch√≠nh x√°c
            token_count = len(cls._tokenizer.encode(test_chunk, add_special_tokens=True))
            
            if token_count > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk = test_chunk
        
        # Add chunk cu·ªëi
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    @classmethod
    def chunk_text_for_ner(cls, text: str, max_chars: int = 1800) -> List[str]:
        """Chia text th√†nh chunks ph√π h·ª£p v·ªõi NER model (max 512 tokens)"""
        # C·∫Øt c√¢u theo ., !, ? v√† c·∫£ d·∫•u k·∫øt th√∫c ki·ªÉu ti·∫øng Vi·ªát
        parts = re.split(r'([.!?„ÄÇ]+)', text)
        # Gh√©p l·∫°i ƒë·ªÉ kh√¥ng m·∫•t d·∫•u ch·∫•m c√¢u
        sentences = []
        for i in range(0, len(parts), 2):
            sent = parts[i].strip()
            punct = parts[i+1] if i+1 < len(parts) else ""
            if sent:
                sentences.append((sent + punct).strip())

        chunks = []
        current = ""
        for sent in sentences:
            if current and len(current) + 1 + len(sent) > max_chars:
                chunks.append(current)
                current = sent
            else:
                current = sent if not current else (current + " " + sent)
        if current:
            chunks.append(current)
        return chunks
    
    @classmethod
    def process_ner(cls, text: str) -> List[Dict[str, Any]]:
        """X·ª≠ l√Ω NER cho vƒÉn b·∫£n, h·ªó tr·ª£ chunking cho vƒÉn b·∫£n d√†i"""
        # Check cache tr∆∞·ªõc
        cache_key = hashlib.md5(text[:1000].encode()).hexdigest()
        if cache_key in cls._ner_cache:
            print("‚úÖ Using cached NER results")
            return cls._ner_cache[cache_key]
        
        # Kh·ªüi t·∫°o client n·∫øu ch∆∞a c√≥
        client = cls._get_ner_client()
        
        # Chia text th√†nh chunks nh·ªè h∆°n ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° gi·ªõi h·∫°n token
        chunks = cls.chunk_text_for_ner(text)
        all_entities = []
        offset = 0
        
        for i, chunk in enumerate(chunks):
            try:
                print(f"üîç Processing NER for chunk {i+1}/{len(chunks)}...")
                
                # G·ªçi API NER
                results = client.token_classification(
                    chunk,
                    model="NlpHUST/ner-vietnamese-electra-base"
                )
                
                # ƒêi·ªÅu ch·ªânh v·ªã tr√≠ v·ªÅ theo vƒÉn b·∫£n g·ªëc
                for entity in results:
                    entity["start"] += offset
                    entity["end"] += offset
                    all_entities.append(entity)
                    
            except Exception as e:
                print(f"‚ö†Ô∏è NER error for chunk {i+1}: {str(e)}")
                continue
                
            # C·∫≠p nh·∫≠t offset cho chunk ti·∫øp theo
            offset += len(chunk) + 1  # +1 cho kho·∫£ng tr·∫Øng
        
        # S·∫Øp x·∫øp v√† g·ªôp c√°c entity li√™n ti·∫øp c√πng lo·∫°i
        all_entities.sort(key=lambda x: (x.get("start", 0), x.get("end", 0)))
        merged_entities = cls._merge_consecutive_entities(all_entities)
        
        # L∆∞u v√†o cache
        cls._ner_cache[cache_key] = merged_entities
        
        return merged_entities
    
    @classmethod
    def _merge_consecutive_entities(cls, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """G·ªôp c√°c entity li√™n ti·∫øp c√πng lo·∫°i"""
        if not entities:
            return entities
            
        merged = []
        current = entities[0].copy()
        
        for entity in entities[1:]:
            # N·∫øu entity li·ªÅn k·ªÅ v√† c√πng lo·∫°i, g·ªôp l·∫°i
            if (entity.get("start") == current.get("end") and 
                entity.get("entity_group") == current.get("entity_group")):
                current["end"] = entity["end"]
                current["word"] = current.get("word", "") + entity.get("word", "")
                # L·∫•y score cao h∆°n
                current["score"] = max(current.get("score", 0), entity.get("score", 0))
            else:
                merged.append(current)
                current = entity.copy()
                
        merged.append(current)
        return merged
    
    @classmethod
    def extract_important_entities(cls, entities: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """Tr√≠ch xu·∫•t c√°c entity quan tr·ªçng theo lo·∫°i"""
        entity_groups = {}
        
        # L·ªçc entity c√≥ score cao v√† ph√¢n lo·∫°i
        for entity in entities:
            if entity.get("score", 0) >= 0.7:  # Ch·ªâ l·∫•y entity c√≥ ƒë·ªô tin c·∫≠y cao
                group = entity.get("entity_group", "OTHER")
                word = entity.get("word", "").strip()
                
                if group not in entity_groups:
                    entity_groups[group] = []
                    
                # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ trong danh s√°ch
                if word and word not in entity_groups[group]:
                    entity_groups[group].append(word)
        
        return entity_groups

    @classmethod
    def summarize_text(cls, text: str, max_length: int = 150) -> str:
        """T√≥m t·∫Øt text v·ªõi API first, model fallback, t√≠ch h·ª£p NER"""
        # Generate cache key ƒë·ªÉ tr√°nh g·ªçi API l·∫∑p l·∫°i
        cache_key = hashlib.md5(text[:1000].encode()).hexdigest()
        
        # Check cache tr∆∞·ªõc
        if cache_key in cls._summary_cache:
            print("‚úÖ Using cached summary")
            return cls._summary_cache[cache_key]
        
        # Clean text
        cleaned_text = re.sub(r'\s+', ' ', text.strip())
        if len(cleaned_text) < 50:
            return "N·ªôi dung qu√° ng·∫Øn ƒë·ªÉ t√≥m t·∫Øt"
        
        # X·ª≠ l√Ω NER ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin quan tr·ªçng
        try:
            print("üîç Extracting named entities...")
            entities = cls.process_ner(cleaned_text)
            important_entities = cls.extract_important_entities(entities)
            
            # T·∫°o context t·ª´ c√°c entity quan tr·ªçng
            entity_context = ""
            for group, words in important_entities.items():
                if words:
                    entity_context += f"{group}: {', '.join(words[:5])}. "
            
            print(f"‚úÖ Extracted entities: {entity_context}")
        except Exception as ner_error:
            print(f"‚ö†Ô∏è NER extraction error: {str(ner_error)}")
            entity_context = ""
            important_entities = {}
        
        # 1. Th·ª≠ d√πng Gemini API
        try:
            print("ü§ñ Trying summarization with Google Gemini API...")
            env_path = Path(__file__).parent.parent.parent / ".env"
            if env_path.exists():
                from dotenv import load_dotenv
                load_dotenv(env_path, override=True)
                
            api_key = settings.GOOGLE_AI_API_KEY_TS
            if not api_key:
                print("‚ö†Ô∏è GOOGLE_AI_API_KEY_TS not found in environment")
                raise ValueError("Google AI API key not set")
                
            # Gi·ªõi h·∫°n ƒë·ªô d√†i vƒÉn b·∫£n ƒë·ªÉ gi·∫£m token
            if len(cleaned_text) > 8000:
                print(f"‚ö†Ô∏è Text too long ({len(cleaned_text)} chars), truncating to 8000 chars")
                cleaned_text = cleaned_text[:8000] + "..."
                
            client = genai.Client(api_key=api_key)
            MODEL_ID = "gemini-1.5-flash"  # Model nh·∫π h∆°n
            
            # C·∫£i thi·ªán prompt v·ªõi th√¥ng tin NER
            prompt = f"""Ban l√† m·ªôt chuy√™n gia trong vi·ªác t√≥m t·∫Øt tin t·ª©c. T√≥m t·∫Øt vƒÉn b·∫£n sau trong kho·∫£ng {max_length} t·ª´, gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng nh·∫•t.
            
            ƒê·∫∑c bi·ªát ch√∫ √Ω ƒë·∫øn c√°c ƒë·ªëi t∆∞·ª£ng quan tr·ªçng sau ƒë√¢y trong vƒÉn b·∫£n:
            {entity_context}

            N·∫øu kh√¥ng c√≥ c√°c ƒë·ªëi t∆∞·ª£ng quan tr·ªçng, h√£y t√≥m t·∫Øt vƒÉn b·∫£n theo c√°ch th√¥ng th∆∞·ªùng.

            TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C B·ªäA ƒê·∫∂T HAY N√ìI B·∫§T C·ª® TH·ª® G√å KH√îNG C√ì TRONG B√ÄI B√ÅO.

            T·∫§T C·∫¢ TH√îNG TIN ƒê·ªÄU PH·∫¢I T·ª™ TIN T·ª®C M√Ä RA.
            
            KH√îNG ƒê∆Ø·ª¢C N√ìI B·∫§T C·ª® TH·ª® G√å KH√îNG C√ì TRONG B√ÄI B√ÅO.
            
            H√£y ƒë·∫£m b·∫£o t√≥m t·∫Øt nh·∫•n m·∫°nh c√°c ƒë·ªëi t∆∞·ª£ng quan tr·ªçng n√†y (con ng∆∞·ªùi, t·ªï ch·ª©c, ƒë·ªãa ƒëi·ªÉm, s·ª± ki·ªán).
            Vi·∫øt t√≥m t·∫Øt ng·∫Øn g·ªçn, s√∫c t√≠ch, d·ªÖ hi·ªÉu, ƒë·ªß √Ω ch√≠nh. VƒÉn b·∫£n:
            
            {cleaned_text}"""

            print(f"üìù Summarizing text of length {len(cleaned_text)}...")

            response = client.models.generate_content(
                model=MODEL_ID,
                contents=prompt
            )
            
            summary = response.text.strip()
            if summary:
                print(f"‚úÖ Gemini API summary: {len(summary)} chars")
                # L∆∞u v√†o cache
                cls._summary_cache[cache_key] = summary
                return summary
            else:
                raise ValueError("Empty summary returned from API")
                
        except Exception as api_error:
            print(f"‚ö†Ô∏è Gemini API error: {str(api_error)}")
            print("üîÑ Falling back to local model...")
            
            # 2. Fallback to local model
            try:
                # Load model if needed
                cls._load_model()
                
                if not cls._model or not cls._tokenizer:
                    raise ValueError("Failed to load local model")
                    
                # Process with local model
                chunks = cls.chunk_text(cleaned_text, max_tokens=256)
                
                if not chunks:
                    raise ValueError("No chunks to summarize")
                
                summaries = []
                
                for i, chunk in enumerate(chunks):
                    try:
                        print(f"üìù Summarizing chunk {i+1}/{len(chunks)} with local model...")
                        
                        # Th√™m entity context v√†o chunk n·∫øu l√† chunk ƒë·∫ßu ti√™n
                        if i == 0 and entity_context:
                            chunk = f"Th√¥ng tin quan tr·ªçng: {entity_context}. {chunk}"
                        
                        inputs = cls._tokenizer(
                            chunk, 
                            return_tensors="pt", 
                            max_length=256,
                            truncation=True, 
                            padding=True
                        ).to(cls._device)
                        
                        # Use lower resource settings
                        with torch.no_grad():
                            summary_ids = cls._model.generate(
                                inputs['input_ids'],
                                attention_mask=inputs['attention_mask'],
                                max_length=min(max_length, 100),
                                min_length=20,
                                num_beams=2,  # Reduced from 3
                                length_penalty=1.0,  # Reduced from 1.2
                                early_stopping=True,
                                no_repeat_ngram_size=2,
                                do_sample=False
                            )
                        
                        summary = cls._tokenizer.decode(
                            summary_ids[0], 
                            skip_special_tokens=True
                        ).strip()
                        
                        if summary and len(summary) > 10:
                            summaries.append(summary)
                        
                    except Exception as chunk_error:
                        print(f"‚ùå Error summarizing chunk {i+1}: {chunk_error}")
                        continue
                
                if not summaries:
                    raise ValueError("No summaries generated")
                
                final_summary = " ".join(summaries)
                final_summary = re.sub(r'\s+', ' ', final_summary).strip()
                
                # H·∫≠u x·ª≠ l√Ω ƒë·ªÉ ƒë·∫£m b·∫£o c√°c entity quan tr·ªçng ƒë∆∞·ª£c nh·∫•n m·∫°nh
                if important_entities:
                    # Th√™m th√¥ng tin v·ªÅ c√°c entity quan tr·ªçng n·∫øu ch∆∞a c√≥ trong t√≥m t·∫Øt
                    for group, words in important_entities.items():
                        if group in ["PERSON", "ORGANIZATION", "LOCATION", "MISCELLANEOUS"]:
                            for word in words[:3]:  # Ch·ªâ l·∫•y 3 entity quan tr·ªçng nh·∫•t m·ªói lo·∫°i
                                if word and len(word) > 1 and word not in final_summary:
                                    # Th√™m v√†o ƒë·∫ßu t√≥m t·∫Øt
                                    final_summary = f"{word} ({group}): {final_summary}"
                                    break
                
                print(f"‚úÖ Local model summary: {len(final_summary)} chars")
                cls._summary_cache[cache_key] = final_summary
                return final_summary
                
            except Exception as local_error:
                print(f"‚ùå Local model error: {str(local_error)}")
                
                # 3. Emergency fallback
                try:
                    # Extract first few sentences
                    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
                    valid_sentences = [s for s in sentences if len(s.strip()) > 10][:3]
                    
                    if valid_sentences:
                        emergency_summary = ". ".join(valid_sentences) + "."
                        print(f"‚ö†Ô∏è Using emergency fallback summary: {len(emergency_summary)} chars")
                        # Kh√¥ng cache emergency summary v√¨ ch·∫•t l∆∞·ª£ng th·∫•p
                        return emergency_summary
                    else:
                        return "Kh√¥ng th·ªÉ t√≥m t·∫Øt n·ªôi dung"
                except Exception as emergency_error:
                    print(f"‚ùå Emergency fallback error: {str(emergency_error)}")
                    return "Kh√¥ng th·ªÉ t√≥m t·∫Øt n·ªôi dung"
    
    # Th√™m ph∆∞∆°ng th·ª©c ƒë·ªÉ qu·∫£n l√Ω cache
    @classmethod
    def clear_old_cache(cls, max_age_minutes=60):
        """Clear cache entries older than max_age_minutes"""
        if not hasattr(cls, "_cache_timestamps"):
            cls._cache_timestamps = {}
        if not hasattr(cls, "_ner_cache_timestamps"):
            cls._ner_cache_timestamps = {}
        
        current_time = datetime.now()
        expired_summary_keys = []
        expired_ner_keys = []
        
        # X·ª≠ l√Ω cache t√≥m t·∫Øt
        for key in cls._summary_cache:
            if key not in cls._cache_timestamps:
                cls._cache_timestamps[key] = current_time
                continue
                
            timestamp = cls._cache_timestamps[key]
            age = current_time - timestamp
            
            if age > timedelta(minutes=max_age_minutes):
                expired_summary_keys.append(key)
        
        # X·ª≠ l√Ω cache NER
        for key in cls._ner_cache:
            if key not in cls._ner_cache_timestamps:
                cls._ner_cache_timestamps[key] = current_time
                continue
                
            timestamp = cls._ner_cache_timestamps[key]
            age = current_time - timestamp
            
            if age > timedelta(minutes=max_age_minutes):
                expired_ner_keys.append(key)
        
        # X√≥a c√°c cache h·∫øt h·∫°n
        for key in expired_summary_keys:
            if key in cls._summary_cache:
                del cls._summary_cache[key]
            if key in cls._cache_timestamps:
                del cls._cache_timestamps[key]
                
        for key in expired_ner_keys:
            if key in cls._ner_cache:
                del cls._ner_cache[key]
            if key in cls._ner_cache_timestamps:
                del cls._ner_cache_timestamps[key]
        
        print(f"üßπ Cleared {len(expired_summary_keys)} old summary cache entries")
        print(f"üßπ Cleared {len(expired_ner_keys)} old NER cache entries")

    @classmethod
    def summarize_article(cls, article: Article, max_length: int = 200) -> str:
        """T√≥m t·∫Øt Article object v·ªõi nh·∫•n m·∫°nh c√°c ƒë·ªëi t∆∞·ª£ng quan tr·ªçng"""
        if not article.blocks:
            return "Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt"
        
        # üîß Extract only paragraph content + filter quality
        paragraphs = []
        for block in article.blocks:
            if (block.type == 'paragraph' and 
                block.content and 
                len(block.content.strip()) > 20):  # Skip very short paragraphs
                paragraphs.append(block.content.strip())
        
        if not paragraphs:
            return "Kh√¥ng c√≥ ƒëo·∫°n vƒÉn ƒë·ªÉ t√≥m t·∫Øt"
        
        full_text = " ".join(paragraphs)
        
        # üîß Add context v·ªõi title
        if article.title:
            full_text = f"Ti√™u ƒë·ªÅ: {article.title}. N·ªôi dung: {full_text}"
        
        return cls.summarize_text(full_text, max_length)
    
    @classmethod
    def cleanup_model(cls):
        """Cleanup model ƒë·ªÉ gi·∫£i ph√≥ng memory"""
        if cls._model is not None:
            del cls._model
            cls._model = None
        if cls._tokenizer is not None:
            del cls._tokenizer
            cls._tokenizer = None
        if cls._ner_client is not None:
            cls._ner_client = None
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        print("üßπ Model cleaned up")